{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b05f5af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msxxxx/miniconda3/envs/env_xx_xx/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading translation model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/msxxxx/miniconda3/envs/env_xx_xx/lib/python3.11/site-packages/transformers/models/marian/tokenization_marian.py:175: UserWarning: Recommended: pip install sacremoses.\n",
      "  warnings.warn(\"Recommended: pip install sacremoses.\")\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translation model loaded successfully!\n",
      "Translating texts to German...\n",
      "Translated 10/100 texts for pool_a\n",
      "Translated 10/100 texts for pool_a\n",
      "Translated 20/100 texts for pool_a\n",
      "Translated 20/100 texts for pool_a\n",
      "Translated 30/100 texts for pool_a\n",
      "Translated 30/100 texts for pool_a\n",
      "Translated 40/100 texts for pool_a\n",
      "Translated 40/100 texts for pool_a\n",
      "Translated 50/100 texts for pool_a\n",
      "Translated 50/100 texts for pool_a\n",
      "Translated 60/100 texts for pool_a\n",
      "Translated 60/100 texts for pool_a\n",
      "Translated 70/100 texts for pool_a\n",
      "Translated 70/100 texts for pool_a\n",
      "Translated 80/100 texts for pool_a\n",
      "Translated 80/100 texts for pool_a\n",
      "Translated 90/100 texts for pool_a\n",
      "Translated 90/100 texts for pool_a\n",
      "Translated 100/100 texts for pool_a\n",
      "Translated 100/100 texts for pool_a\n",
      "Translated 10/100 texts for pool_b\n",
      "Translated 10/100 texts for pool_b\n",
      "Translated 20/100 texts for pool_b\n",
      "Translated 20/100 texts for pool_b\n",
      "Translated 30/100 texts for pool_b\n",
      "Translated 30/100 texts for pool_b\n",
      "Translated 40/100 texts for pool_b\n",
      "Translated 40/100 texts for pool_b\n",
      "Translated 50/100 texts for pool_b\n",
      "Translated 50/100 texts for pool_b\n",
      "Translated 60/100 texts for pool_b\n",
      "Translated 60/100 texts for pool_b\n",
      "Translated 70/100 texts for pool_b\n",
      "Translated 70/100 texts for pool_b\n",
      "Translated 80/100 texts for pool_b\n",
      "Translated 80/100 texts for pool_b\n",
      "Translated 90/100 texts for pool_b\n",
      "Translated 90/100 texts for pool_b\n",
      "Translated 100/100 texts for pool_b\n",
      "Successfully saved pool_a.json and pool_b.json with German translations\n",
      "Translated 100/100 texts for pool_b\n",
      "Successfully saved pool_a.json and pool_b.json with German translations\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import pipeline\n",
    "\n",
    "def load_data(dataset_name, split, task):\n",
    "    examples = []\n",
    "    task_str = \"tasd\" if task in ['tasd', 'acd', 'e2e'] else task\n",
    "    with open(f\"data/{task_str}/{dataset_name}/{split}.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            text, aspect_str = line.strip().split(\"####\")\n",
    "            aspect_list = eval(aspect_str)  # besser w√§re ast.literal_eval\n",
    "            \n",
    "            if considered_sentiment_elements == [\"aspect_term\", \"aspect_category\", \"sentiment_polarity\", \"opinion_term\"]:\n",
    "                aspect_list = [\n",
    "                    {\n",
    "                        \"aspect_term\": aspect[0],\n",
    "                        \"aspect_category\": aspect[1],\n",
    "                        \"sentiment_polarity\": aspect[2],\n",
    "                        \"opinion_term\": aspect[3]\n",
    "                    }\n",
    "                    for aspect in aspect_list\n",
    "                ]\n",
    "            elif considered_sentiment_elements == [\"aspect_category\"]:\n",
    "                aspect_list = [\n",
    "                    {\n",
    "                        \"aspect_category\": aspect[1]\n",
    "                    }\n",
    "                    for aspect in aspect_list\n",
    "                ]\n",
    "                # remove duplicates in aspect_list\n",
    "                aspect_list = [dict(t) for t in {tuple(d.items()) for d in aspect_list}]\n",
    "            elif considered_sentiment_elements == [\"aspect_term\", \"aspect_category\", \"sentiment_polarity\"]:\n",
    "                aspect_list = [\n",
    "                    {\n",
    "                        \"aspect_term\": aspect[0],\n",
    "                        \"aspect_category\": aspect[1],\n",
    "                        \"sentiment_polarity\": aspect[2]\n",
    "                    }\n",
    "                    for aspect in aspect_list\n",
    "                ]\n",
    "\n",
    "            examples.append({\n",
    "                \"text\": text,\n",
    "                \"label\": aspect_list\n",
    "            })\n",
    "    return examples\n",
    "\n",
    "# Initialize the translation pipeline with a Hugging Face model\n",
    "print(\"Loading translation model...\")\n",
    "translator = pipeline(\"translation_en_to_de\", model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "print(\"Translation model loaded successfully!\")\n",
    "\n",
    "def translate_to_german(text):\n",
    "    \"\"\"Translate text to German using Hugging Face model\"\"\"\n",
    "    try:\n",
    "        # The model expects a list of texts\n",
    "        result = translator(text, max_length=512)\n",
    "        return result[0]['translation_text']\n",
    "    except Exception as e:\n",
    "        print(f\"Translation error for '{text}': {e}\")\n",
    "        return text  # Return original text if translation fails\n",
    "\n",
    "considered_sentiment_elements=[\"aspect_term\", \"aspect_category\", \"sentiment_polarity\", \"opinion_term\"]\n",
    "train_data = load_data(\"rest16\", \"train\", \"asqp\")\n",
    "pool_a = train_data[:100]\n",
    "pool_b = train_data[100:200]\n",
    "len(pool_a), len(pool_b)\n",
    "\n",
    "# Prepare data for saving (remove 'label' key and add 'translation' key)\n",
    "print(\"Translating texts to German...\")\n",
    "\n",
    "pool_a_processed = []\n",
    "for i, item in enumerate(pool_a):\n",
    "    translation = translate_to_german(item[\"text\"])\n",
    "    pool_a_processed.append({\n",
    "        \"text\": item[\"text\"],\n",
    "        \"translation\": translation\n",
    "    })\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Translated {i + 1}/100 texts for pool_a\")\n",
    "\n",
    "pool_b_processed = []\n",
    "for i, item in enumerate(pool_b):\n",
    "    translation = translate_to_german(item[\"text\"])\n",
    "    pool_b_processed.append({\n",
    "        \"text\": item[\"text\"],\n",
    "        \"translation\": translation\n",
    "    })\n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"Translated {i + 1}/100 texts for pool_b\")\n",
    "\n",
    "# Save pool_a.json and pool_b.json\n",
    "with open(\"pool_a.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pool_a_processed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"pool_b.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(pool_b_processed, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"Successfully saved pool_a.json and pool_b.json with German translations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env_xx_xx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
